{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7661190,"sourceType":"datasetVersion","datasetId":4467133},{"sourceId":7809564,"sourceType":"datasetVersion","datasetId":4573969}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install sentence_transformers\n!pip install chromadb\n!pip install peft","metadata":{"execution":{"iopub.status.busy":"2024-03-16T17:17:02.675724Z","iopub.execute_input":"2024-03-16T17:17:02.676090Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Collecting sentence_transformers\n  Downloading sentence_transformers-2.5.1-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.32.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.38.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.66.1)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.26.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.11.4)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.20.3)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.9.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (21.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.4.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.15.1->sentence_transformers) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\nDownloading sentence_transformers-2.5.1-py3-none-any.whl (156 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence_transformers\nSuccessfully installed sentence_transformers-2.5.1\nCollecting chromadb\n  Downloading chromadb-0.4.24-py3-none-any.whl.metadata (7.3 kB)\nCollecting build>=1.0.3 (from chromadb)\n  Downloading build-1.1.1-py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: requests>=2.28 in /opt/conda/lib/python3.10/site-packages (from chromadb) (2.31.0)\nRequirement already satisfied: pydantic>=1.9 in /opt/conda/lib/python3.10/site-packages (from chromadb) (2.5.3)\nCollecting chroma-hnswlib==0.7.3 (from chromadb)\n  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\nRequirement already satisfied: fastapi>=0.95.2 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.108.0)\nRequirement already satisfied: uvicorn>=0.18.3 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.25.0)\nRequirement already satisfied: numpy>=1.22.5 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.26.4)\nCollecting posthog>=2.4.0 (from chromadb)\n  Downloading posthog-3.5.0-py2.py3-none-any.whl.metadata (2.0 kB)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.9.0)\nCollecting pulsar-client>=3.1.0 (from chromadb)\n  Downloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.0 kB)\nCollecting onnxruntime>=1.14.1 (from chromadb)\n  Downloading onnxruntime-1.17.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\nRequirement already satisfied: opentelemetry-api>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.22.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.22.0)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n  Downloading opentelemetry_instrumentation_fastapi-0.44b0-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.22.0)\nRequirement already satisfied: tokenizers>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.15.2)\nCollecting pypika>=0.48.9 (from chromadb)\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m675.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l-","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report\nfrom sentence_transformers import SentenceTransformer\nimport pandas as pd\nfrom chromadb.utils import embedding_functions\nfrom tqdm import tqdm\nimport chromadb\nimport torch\nfrom transformers import Trainer\nfrom transformers import BertTokenizer, BertForSequenceClassification\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-03-16T17:19:06.200793Z","iopub.execute_input":"2024-03-16T17:19:06.201477Z","iopub.status.idle":"2024-03-16T17:19:06.207251Z","shell.execute_reply.started":"2024-03-16T17:19:06.201440Z","shell.execute_reply":"2024-03-16T17:19:06.206217Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"device = 'CUDA' if torch.cuda.is_available else 'CPU'\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-03-16T17:19:12.808926Z","iopub.execute_input":"2024-03-16T17:19:12.809389Z","iopub.status.idle":"2024-03-16T17:19:12.815428Z","shell.execute_reply.started":"2024-03-16T17:19:12.809359Z","shell.execute_reply":"2024-03-16T17:19:12.814545Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'CUDA'"},"metadata":{}}]},{"cell_type":"code","source":"import torch\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)","metadata":{"execution":{"iopub.status.busy":"2024-03-16T17:19:13.585874Z","iopub.execute_input":"2024-03-16T17:19:13.586735Z","iopub.status.idle":"2024-03-16T17:19:13.591230Z","shell.execute_reply.started":"2024-03-16T17:19:13.586704Z","shell.execute_reply":"2024-03-16T17:19:13.590208Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"<h2>Загрузка моделей LLM, retriver, intent recognition</h2>","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n\nllm_model = AutoModelForCausalLM.from_pretrained(\"openchat/openchat-3.5-0106\", device_map=\"auto\", torch_dtype=\"auto\")\nllm_tokenizer = AutoTokenizer.from_pretrained(\"openchat/openchat-3.5-0106\", use_fast=False)\ngeneration_config = GenerationConfig.from_pretrained('openchat/openchat-3.5-0106')\ngeneration_config.top_k= 40\ngeneration_config.top_p= 0.9\ngeneration_config.temperature= 0.3\ngeneration_config.do_sample= True","metadata":{"execution":{"iopub.status.busy":"2024-03-16T17:19:15.894255Z","iopub.execute_input":"2024-03-16T17:19:15.894879Z","iopub.status.idle":"2024-03-16T17:21:18.391840Z","shell.execute_reply.started":"2024-03-16T17:19:15.894849Z","shell.execute_reply":"2024-03-16T17:21:18.391101Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/651 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cd2dbb9cd884ec6a8ed3e79e7377325"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a99724e03754af9a20d8ef7a1ccec51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f8a05a1ad0748bc95e597a04982880f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fdc03c1c720485fb2f562ffabf7b70b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1928678abe7948088927dcf16e0898f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a55c8d36f0414b859de9c088a950bebb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff2722e4ebee4ecea098fa38f6aa09fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/179 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bbeb99ce6d1426ea19a9f2e12fe7664"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7dc9581dca14ff89de42ba08d1698b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e195b79f47994465a0f1c4e3d04cf52c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a7c3fadca6b40de9e70670f7e6edd31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/491 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24c81222f1354fa3b4a6982ecf31058e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f858a51dac348e2b1a50b6ef34d737f"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"retriver_model = SentenceTransformer(\"intfloat/multilingual-e5-small\")\n# retriver_model = SentenceTransformer(\"hivaze/ru-e5-large\")","metadata":{"execution":{"iopub.status.busy":"2024-03-16T17:22:34.887957Z","iopub.execute_input":"2024-03-16T17:22:34.888747Z","iopub.status.idle":"2024-03-16T17:22:41.311700Z","shell.execute_reply.started":"2024-03-16T17:22:34.888715Z","shell.execute_reply":"2024-03-16T17:22:41.310907Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef7715bafe2841ca864ac3644fa7830d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/160k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e2db7e521fc4bb4bdfd13f830b96403"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3be2f03c7b4343df96937db2c10e8291"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/655 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7df9e0a3e4734a76b42fceb4384d42d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e594d89ee5f240e3940052b56a16f5db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2369f82abd1d458297784105050bfca7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f4c3215a6434fba89cd0f1880fc3877"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f8dc37ae6d94c118edc89a706efc37e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/167 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdeb5a8b876a4d71b6aace28b8d5a67b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cc1c315bfc1488d8308a8a28ab2e2d1"}},"metadata":{}}]},{"cell_type":"code","source":"BASE_MODEL_NAME = \"cointegrated/rubert-tiny2\"\nMODEL_NAME = \"/kaggle/input/intent-model/intent-model\"\nintent_model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3)\nintent_tokenizer = BertTokenizer.from_pretrained(BASE_MODEL_NAME)","metadata":{"execution":{"iopub.status.busy":"2024-03-16T17:22:41.313444Z","iopub.execute_input":"2024-03-16T17:22:41.313774Z","iopub.status.idle":"2024-03-16T17:22:43.413346Z","shell.execute_reply.started":"2024-03-16T17:22:41.313747Z","shell.execute_reply":"2024-03-16T17:22:43.412449Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/401 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b68781e7573741a2972c58f72cc05f26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/1.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95bfc7343de740838b63d812232be4a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f452f98332b4b3f9c44fed924dc09d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.74M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9abdc462c58649bcad22b0f7cfac156f"}},"metadata":{}}]},{"cell_type":"code","source":"# !chroma run --path /db_path","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Инициализация векторной базы данных, загрузка и векторизация данных</h2>","metadata":{}},{"cell_type":"code","source":"client = chromadb.PersistentClient(path=\"/database/data\")\nprint('База данных подключена') \n# invest_qa_emb = client.create_collection(name=db_name, metadata={'hnsw:space': 'cosine'})\ncollection = client.create_collection(name=\"Invest_collection\", embedding_function=embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"intfloat/multilingual-e5-small\"), metadata={'hnsw:space': 'cosine'})\nprint('Конфигурация коллекции создана')\ndf = pd.read_excel('/kaggle/input/data-with-pair-qa/data.xlsx')\ncollection.add(\n    documents=df.Answer.to_list(),\n    metadatas=[{'question': q} for q in df.Theme.to_list()],\n    ids=[str(i) for i in range(len(df))]\n)\nprint('Данные загружены')","metadata":{"execution":{"iopub.status.busy":"2024-03-16T17:22:43.414418Z","iopub.execute_input":"2024-03-16T17:22:43.414709Z","iopub.status.idle":"2024-03-16T17:23:23.755363Z","shell.execute_reply.started":"2024-03-16T17:22:43.414684Z","shell.execute_reply":"2024-03-16T17:23:23.754364Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"База данных подключена\nКонфигурация коллекции создана\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/45 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15d5eb1e27714afcba2ea966916b87c9"}},"metadata":{}},{"name":"stdout","text":"Данные загружены\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Временно не нужно","metadata":{}},{"cell_type":"code","source":"# import torch\n# from peft import PeftModel, PeftConfig\n# from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n\n\n# MODEL_NAME = \"IlyaGusev/saiga_mistral_7b\"\n# config = PeftConfig.from_pretrained(MODEL_NAME)\n# model = AutoModelForCausalLM.from_pretrained(\n#     config.base_model_name_or_path,\n#     load_in_8bit=False,\n#     torch_dtype=torch.float16,\n#     device_map=\"auto\"\n# )\n# model = PeftModel.from_pretrained(\n#     model,\n#     MODEL_NAME,\n#     torch_dtype=torch.float16\n# )\n# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n# generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n# print(generation_config)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Создания класса Conversation для инициализации промпта, хранение контекста</h2>","metadata":{}},{"cell_type":"code","source":"# DEFAULT_MESSAGE_TEMPLATE = \"<s>{role}\\n{content}</s>\"\n# DEFAULT_RESPONSE_TEMPLATE = \"<s>bot\\n\"\nDEFAULT_MESSAGE_TEMPLATE = \"<|{role}|>\\n{content}</s>\\n\"\nDEFAULT_RESPONSE_TEMPLATE = \"<|assistant|>\\n\"\nDEFAULT_SYSTEM_PROMPT = \"Ты интеллектуальный помощник по инвестициям.Ты должен отвечать на вопросы пользователей на РУССКОМ ЯЗЫКЕ, основываясь на предоставленных ответах.\"\nclass Conversation:\n    def __init__(\n        self,\n        message_template=DEFAULT_MESSAGE_TEMPLATE,\n        system_prompt=DEFAULT_SYSTEM_PROMPT,\n        response_template=DEFAULT_RESPONSE_TEMPLATE,\n        default_conversation_len = 17\n    ):\n        self.message_template = message_template\n        self.response_template = response_template\n        self.messages = [{\n            \"role\": \"system\",\n            \"content\": system_prompt\n        }]\n        self.default_conversation_len = default_conversation_len\n    def check_len(self):\n        if self.__len__()==self.default_conversation_len:\n                del self.messages[1]\n#                 del self.messages[1]\n    def add_user_message(self, message):\n        self.check_len()\n        self.messages.append({\n            \"role\": \"user\",\n            \"content\": message\n        })\n    def add_answer_message(self, message):\n        self.check_len()\n        self.messages.append({\n            \"role\": \"user\",\n            \"content\": message\n        })\n\n    def add_bot_message(self, message):\n        self.check_len()\n        self.messages.append({\n            \"role\": \"bot\",\n            \"content\": message\n        })\n    def get_prompt(self, tokenizer):\n        final_text = \"\"\n        for message in self.messages:\n            message_text = self.message_template.format(**message)\n            final_text += message_text\n        final_text += DEFAULT_RESPONSE_TEMPLATE\n        return final_text.strip()\n    def __len__(self):\n        return len(self.messages)\ndef generate(model, tokenizer, prompt, generation_config):\n    data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\")\n    data = {k: v.to(\"cuda\") for k, v in data.items()}\n    output_ids = model.generate(\n        **data,\n        generation_config=generation_config\n    )[0]\n    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n    return output.strip()","metadata":{"execution":{"iopub.status.busy":"2024-03-16T18:09:59.721852Z","iopub.execute_input":"2024-03-16T18:09:59.722274Z","iopub.status.idle":"2024-03-16T18:09:59.735344Z","shell.execute_reply.started":"2024-03-16T18:09:59.722244Z","shell.execute_reply":"2024-03-16T18:09:59.734248Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"markdown","source":"<h2>Функция генерации ответа, которая состоит из</h2>\n<h3>1) Доставание 5 ближайщих соседей к вопросу</h3>\n<h3>2) Классификация вопроса на 3 класса (0-полезный запрос, 1- запрос-доспрашивание, 2-вопрос не относящийся к теме инвестиций)</h3>\n<h3>3) Генерация ответа в зависимости от класса вопроса</h3>","metadata":{}},{"cell_type":"code","source":"def get_answer(query,conversation):\n    DEFAULT_USER_MESSAGE= '''Тебе будут предостоставлены ответы на похожие вопросы из базы знаний в формате (ответ: коэфициент релевантности вопросу пользователя). Если среди предоставленных ответов есть информация которая отвечает на вопрос, то составь красивый и понятный ответ для пользователя, а если среди ответов нет информации, которая отвечает на вопрос, то переспроси пользователя(уточни что он имел ввиду), ни в коем случае не придумывай информацию!'''\n    DEFAULT_USER_FURTHER_QUESTIONING = '''Нужно объяснить более понятным языком, основываясь на ответы из базы данных'''\n    def get_answers_from_db(query):\n        collection = client.get_collection(\"Invest_collection\")\n        input_em = retriver_model.encode(sentences=f\"query: {query}\", batch_size=1).tolist()\n        results = collection.query(\n            query_embeddings=[input_em],\n            n_results=5\n        )\n        answers = dict(zip(results['documents'][0], results['distances'][0]))\n        for key, value in answers.items():\n            answers[key] = 1 - value\n#         answers = [key for key in answers.keys()]\n        return answers\n    class QueryDataset(torch.utils.data.Dataset):\n        def __init__(self, encodings, labels=None):\n            self.encodings = encodings\n            self.labels = labels\n\n        def __getitem__(self, idx):\n            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n            if self.labels:\n                item[\"labels\"] = torch.tensor(self.labels[idx])\n            return item\n\n        def __len__(self):\n            return len(self.encodings[\"input_ids\"])\n    tokenized_query = intent_tokenizer([query], padding=True, truncation=True, max_length=512)\n    tokenized_query_dataset = QueryDataset(encodings=tokenized_query)\n    test_trainer = Trainer(intent_model)\n    raw_pred, _, _ = test_trainer.predict(tokenized_query_dataset)\n    y_pred = torch.argmax(torch.softmax(torch.tensor(raw_pred), dim=-1), axis=1)\n    print(y_pred)\n    if y_pred==0:\n        answers = get_answers_from_db(query)\n        prompt_query = DEFAULT_USER_MESSAGE+query+str(answers)\n        conversation.add_user_message(prompt_query)\n        prompt = conversation.get_prompt(llm_tokenizer)\n        output = generate(llm_model, llm_tokenizer, prompt, generation_config)\n        torch.cuda.empty_cache()\n        return output\n    elif y_pred==1:\n        prompt_query = DEFAULT_USER_FURTHER_QUESTIONING+query\n        conversation.add_user_message(prompt_query)   \n        prompt = conversation.get_prompt(llm_tokenizer)\n        output = generate(llm_model, llm_tokenizer, prompt, generation_config)\n        torch.cuda.empty_cache()\n        return output\n    else:\n        return 'Пожалуйста повторите ваш вопрос'","metadata":{"execution":{"iopub.status.busy":"2024-03-16T18:10:31.963802Z","iopub.execute_input":"2024-03-16T18:10:31.964668Z","iopub.status.idle":"2024-03-16T18:10:31.978302Z","shell.execute_reply.started":"2024-03-16T18:10:31.964633Z","shell.execute_reply":"2024-03-16T18:10:31.977369Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"get_answer('Я не совсем понял',conversation)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Запуск телеграм бота и использование функции генерации ответа</h2>","metadata":{}},{"cell_type":"code","source":"!pip install aiogram==2.25.2\n!pip install nest-asyncio\n# !pip install aiohttp==3.8.6","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import logging\nfrom aiogram import Bot, Dispatcher, types\nfrom aiogram.dispatcher.filters.state import State, StatesGroup\nfrom aiogram.contrib.middlewares.logging import LoggingMiddleware\nfrom aiogram.types import InlineKeyboardButton, InlineKeyboardMarkup\nfrom aiogram.types import ReplyKeyboardMarkup, KeyboardButton\n# from aiogram.utils import executor\n\n# Инициализация бота и диспетчера\nAPI_TOKEN = '7093361951:AAFDy4EkMv6pIB7MdlLsJKSKdLYcOVxcBjE'\nbot = Bot(token=API_TOKEN)\ndp = Dispatcher(bot)\nlogging.basicConfig(level=logging.INFO)\ndp.middleware.setup(LoggingMiddleware())\n\nuser_dialogs = {}\nuser_context = {}\n\n# Обработчик команды /start\n@dp.message_handler(commands=['start'])\nasync def start(message: types.Message):\n    user_id = message.from_user.id\n    user_dialogs[user_id] = []  # Создаем пустой диалог для пользователя\n    keyboard = ReplyKeyboardMarkup(resize_keyboard=True).add(KeyboardButton(\"ОБЪЯСНИ\"))\n    await message.answer(\"Привет! Я диалоговый бот. Нажми кнопку 'ОБЪЯСНИ', чтобы начать диалог.\", reply_markup=keyboard)\n\n# Обработчик кнопки \"ОБЪЯСНИ\"\n@dp.message_handler(lambda message: message.text == 'ОБЪЯСНИ')\nasync def explain(message: types.Message, state: FSMContext):\n    user_id = message.from_user.id\n    user_dialogs[user_id].append([\"юзер\", message.text])  # Добавляем сообщение пользователя в диалог\n    user_context[user_id] = Conversation()\n    keyboard = ReplyKeyboardMarkup(resize_keyboard=True).add(KeyboardButton(\"ПОНЯЛ\"))\n    await message.answer(\"Теперь я могу объяснить. Напиши свой вопрос:\", reply_markup=keyboard)\n\n# Обработчик всех сообщений внутри диалога\n@dp.message_handler(lambda message: message.text != 'ПОНЯЛ', state=\"*\")\nasync def handle_dialog(message: types.Message, state: FSMContext):\n    user_id = message.from_user.id\n    conversation = user_context.get(user_id)\n    print(conversation)\n    await message.answer('Ваш вопрос принят, ожидайте')\n    answer = get_answer(message.text,conversation)\n    print('Ответ готов')\n    user_dialogs[user_id].append([\"юзер\", message.text])  # Добавляем сообщение пользователя в диалог\n    user_dialogs[user_id].append(['бот', answer])\n    await message.answer(answer)\n    await message.answer(\"Продолжай задавать вопросы или нажми кнопку 'ПОНЯЛ', чтобы завершить диалог.\")\n\n# Обработчик кнопки \"ПОНЯЛ\"\n@dp.message_handler(lambda message: message.text == 'ПОНЯЛ')\nasync def understood(message: types.Message):\n    user_id = message.from_user.id\n    dialog = user_dialogs.get(user_id, [])\n    # Сохраняем диалог в файл (вместо этого можно использовать базу данных)\n    with open(f\"user_{user_id}_dialog.txt\", \"w\", encoding='utf-8') as file:\n        for entry in dialog:\n            file.write(f\"{entry[0]}: {entry[1]}\\n\")\n    user_dialogs[user_id] = []  # Обнуляем диалог\n    conversation = Conversation()\n    keyboard = ReplyKeyboardMarkup(resize_keyboard=True).add(KeyboardButton(\"ОБЪЯСНИ\"))\n    await message.answer(\"Диалог сохранен. Начнем заново!\", reply_markup=keyboard)\n\n\nimport nest_asyncio\nnest_asyncio.apply()\nif __name__ == '__main__':\n    from aiogram import executor\n    executor.start_polling(dp, skip_updates=True)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install telebot","metadata":{"execution":{"iopub.status.busy":"2024-03-16T17:41:39.365346Z","iopub.execute_input":"2024-03-16T17:41:39.365718Z","iopub.status.idle":"2024-03-16T17:41:52.774352Z","shell.execute_reply.started":"2024-03-16T17:41:39.365689Z","shell.execute_reply":"2024-03-16T17:41:52.773133Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting telebot\n  Downloading telebot-0.0.5-py3-none-any.whl.metadata (2.0 kB)\nCollecting pyTelegramBotAPI (from telebot)\n  Downloading pytelegrambotapi-4.16.1-py3-none-any.whl.metadata (47 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 kB\u001b[0m \u001b[31m688.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from telebot) (2.31.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->telebot) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->telebot) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->telebot) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->telebot) (2024.2.2)\nDownloading telebot-0.0.5-py3-none-any.whl (4.8 kB)\nDownloading pytelegrambotapi-4.16.1-py3-none-any.whl (232 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.2/232.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyTelegramBotAPI, telebot\nSuccessfully installed pyTelegramBotAPI-4.16.1 telebot-0.0.5\n","output_type":"stream"}]},{"cell_type":"code","source":"import telebot\nimport logging\nimport random\n\nAPI_TOKEN = '7093361951:AAFDy4EkMv6pIB7MdlLsJKSKdLYcOVxcBjE'\nbot = telebot.TeleBot(API_TOKEN)\n\nuser_dialogs = {}\nuser_context = {}\n\n# Обработчик команды /start\n@bot.message_handler(commands=['start'])\ndef start(message):\n    user_id = message.from_user.id\n    user_dialogs[user_id] = []  # Создаем пустой диалог для пользователя\n    keyboard = telebot.types.ReplyKeyboardMarkup(resize_keyboard=True).add(telebot.types.KeyboardButton(\"Новый чат\"))\n    user_id = message.from_user.id\n    user_dialogs[user_id].append([\"юзер\", message.text])\n    user_context[user_id] = Conversation()\n    bot.send_message(message.chat.id, \"Привет! Я ваш инвестиционный помощник. Задавай свой вопрос и я постараюсь объяснить мир инвестиций максимально простым языком.\", reply_markup=keyboard)\n\n# Обработчик всех сообщений внутри диалога\n@bot.message_handler(func=lambda message: message.text != 'Новый чат', content_types=['text'])\ndef handle_dialog(message):\n    user_id = message.from_user.id\n    conversation = user_context.get(user_id)\n    print(len(conversation))\n    answer = get_answer(message.text,conversation)\n    conversation.add_bot_message(answer)\n    print('Ответ готов')\n    user_dialogs[user_id].append([\"юзер\", message.text])\n    user_dialogs[user_id].append(['бот', answer])\n    bot.send_message(message.chat.id, answer)\n#     bot.send_message(message.chat.id, \"Продолжай задавать вопросы или нажми кнопку 'ПОНЯЛ', чтобы завершить диалог.\")\n\n# Обработчик кнопки \"ПОНЯЛ\"\n@bot.message_handler(func=lambda message: message.text == 'Новый чат')\ndef understood(message):\n    user_id = message.from_user.id\n    dialog = user_dialogs.get(user_id, [])\n    # Сохраняем диалог в файл (вместо этого можно использовать базу данных)\n    with open(f\"user_{user_id}_dialog.txt\", \"w\", encoding='utf-8') as file:\n        for entry in dialog:\n            file.write(f\"{entry[0]}: {entry[1]}\\n\")\n    user_dialogs[user_id] = []  # Обнуляем диалог\n    conversation = Conversation()\n    print(len(conversation))\n    keyboard = telebot.types.ReplyKeyboardMarkup(resize_keyboard=True).add(telebot.types.KeyboardButton(\"Новый чат\"))\n    bot_message = ['Начинаем с чистого листа', 'Разговаривал бы с вами вечно', 'Что нибудь еще?', 'Спрашивайте, мне нравится вам помогать!', \"Ух, я готов вам помочь\"]\n    bot.send_message(message.chat.id, random.choice(bot_message), reply_markup=keyboard)\n\nif __name__ == '__main__':\n    bot.polling(none_stop=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-16T18:11:12.749780Z","iopub.execute_input":"2024-03-16T18:11:12.750205Z","iopub.status.idle":"2024-03-16T18:21:42.540457Z","shell.execute_reply.started":"2024-03-16T18:11:12.750173Z","shell.execute_reply":"2024-03-16T18:21:42.539580Z"},"trusted":true},"execution_count":94,"outputs":[{"name":"stdout","text":"1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"tensor([0])\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77408bd626554c80897247eb7f03d873"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Ответ готов\n3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"tensor([0])\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"804134fe871443d1bfccf7449074ea96"}},"metadata":{}},{"name":"stdout","text":"Ответ готов\n5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"tensor([1])\nОтвет готов\n7\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"tensor([1])\nОтвет готов\n1\n1\n1\n1\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}